---
description:
globs:
alwaysApply: true
---
Excellent. I’ll design a comprehensive project structure that fully leverages FastAPI’s features—covering ML model serving, real-time dashboards, internal tools, async tasks, background processing, integrations with databases and external APIs, dependency injection, custom middleware, WebSocket support, and full user authentication with RBAC.

I'll get started and let you know when it's ready to explore.


# Advanced FastAPI Project Structure for Machine Learning Applications

**Overview:** This project structure is designed for a Machine Learning Engineer to **serve ML models**, provide a **real-time dashboard**, and build **internal tooling** using FastAPI. It emphasizes asynchronous design, background processing, multi-database integration, external API calls, and robust authentication/authorization. A clear and layered structure ensures the codebase is **modular, scalable, and maintainable**, which in turn simplifies collaboration, testing, and deployment. The following sections outline a recommended directory layout and explain how advanced FastAPI features are applied throughout the project.

## Project Directory Structure

Below is a high-level view of the project’s directory organization. Each folder/module has a distinct purpose, enabling separation of concerns and ease of navigation:

```bash
my_fastapi_ml_project/
├── app/
│   ├── __init__.py
│   ├── main.py                # Application entry point (FastAPI app, events, middleware, routers)
│   ├── api/                   # API package (routers, dependencies, versioning)
│   │   ├── __init__.py
│   │   ├── dependencies.py    # Reusable dependency functions (e.g., get_db, auth handlers)
│   │   └── routers/           # Modular route definitions
│   │       ├── __init__.py
│   │       ├── auth.py        # Auth & token endpoints (login, signup, refresh)
│   │       ├── users.py       # User management endpoints
│   │       ├── ml_model.py    # ML model serving endpoints (prediction, status)
│   │       ├── dashboard.py   # Dashboard endpoints (analytics, SSE/WebSocket streams)
│   │       └── internal.py    # Internal tooling endpoints (admin tasks, etc.)
│   ├── core/                  # Core configuration, security, and app-wide utilities
│   │   ├── __init__.py
│   │   ├── config.py          # Configuration settings (Pydantic BaseSettings for env vars)
│   │   ├── security.py        # Security utils (JWT token creation/validation, password hashing)
│   │   ├── middleware.py      # Custom middleware (e.g., logging, timing, CORS, RBAC enforcement)
│   │   └── rbac.py            # Role-based access control logic (if not in middleware)
│   ├── models/                # ORM database models (SQLModel/SQLAlchemy classes)
│   │   ├── __init__.py
│   │   ├── user.py            # User model (with roles/permissions fields)
│   │   ├── item.py            # Example domain model (if needed for internal tools)
│   │   └── ...
│   ├── schemas/               # Pydantic models for requests and responses
│   │   ├── __init__.py
│   │   ├── user.py            # Pydantic schemas for User (UserCreate, UserRead, etc.)
│   │   ├── ml.py              # Schemas for ML input payloads and prediction results
│   │   └── dashboard.py       # Schemas for dashboard data (if any)
│   ├── db/                    # Database configuration and sessions
│   │   ├── __init__.py
│   │   ├── session.py         # SQLAlchemy/SQLModel session, engine setup (PostgreSQL connection)
│   │   ├── redis.py           # Redis client setup (for caching, task queue)
│   │   └── migrations/        # DB migration scripts (e.g., Alembic)
│   ├── services/              # Business logic, integrations, and long-running tasks
│   │   ├── __init__.py
│   │   ├── ml_service.py      # ML model loading, inference logic (could use BackgroundTasks/Celery)
│   │   ├── dashboard_service.py # Real-time data aggregation for dashboard, WebSocket broadcast logic
│   │   ├── auth_service.py    # User auth helpers (if not using external library)
│   │   ├── external_api.py    # External API client calls (using httpx for async requests)
│   │   └── tasks.py           # Background task functions (e.g., Celery tasks definitions)
│   └── middleware/ (optional) # If multiple middleware components, could be a package
│       ├── __init__.py
│       └── rbac_middleware.py # Example: middleware class enforcing RBAC on requests
├── tests/                     # Test suite for unit and integration tests
│   ├── __init__.py
│   ├── test_auth.py
│   ├── test_ml_model.py
│   ├── test_dashboard.py
│   └── ...
├── .env                       # Environment variables for development (never commit secrets)
├── pyproject.toml or requirements.txt  # Project dependencies
├── Dockerfile                 # Containerization configuration (for deployment)
└── README.md                  # Project documentation
```

**Structure Rationale:** This layout separates concerns into logical modules, making it easy to scale and maintain. For example, **API routers** are grouped by feature domain (auth, users, ML model, dashboard, internal admin) under `app/api/routers`, each encapsulating a set of related endpoints. The **core** directory centralizes configuration, security, and cross-cutting concerns (like middleware). **Models** and **schemas** distinguish database persistence models from Pydantic validation schemas, a common practice for clarity. The **services** layer holds business logic and integration code, helping keep route functions lean. Such separation follows best practices to keep the codebase *clean, scalable, and testable*.

Next, we dive into each major component, describing how key FastAPI features are utilized and how the pieces fit together.

## Main Application Setup (`app/main.py`)

The `main.py` is the **entry point** of the application. Here we create the FastAPI app and include all routers, configure middleware, and set up startup/shutdown events:

* **App Initialization:** We instantiate `app = FastAPI()` in this module, possibly with metadata like title, description, and version. We also mount sub-apps or include routers for each API module. For example, `app.include_router(api_router, prefix="/api/v1")` could be used to include all routers from the `app/api` package (potentially under a versioned prefix like `/api/v1`).

* **Dependency Injection Wiring:** We add global dependencies or event handlers here if needed. For instance, if using *lifespan* context (FastAPI 0.95+), we might define an `@asynccontextmanager` function and pass it to `FastAPI(lifespan=lifespan)` to manage startup/shutdown tasks (like connecting to DB or loading models). Alternatively, we use `@app.on_event("startup")` and `@app.on_event("shutdown")` decorators to run initialization and teardown code. In this project, startup events are used to **load ML models into memory, open database connections, and initialize external clients** before serving requests. This ensures expensive operations (like loading a large model from disk) occur only once at startup, not on every request. For example, we might load a PyTorch or TensorFlow model and store it in `app.state` or a global variable during startup.

* **Middleware Registration:** We attach any custom middleware in `main.py` via `app.add_middleware`. Middleware classes (like those in `app/core/middleware.py`) can log requests, handle CORS, or enforce RBAC globally. For instance, we might add Starlette’s `CORSMiddleware` to allow a frontend dashboard to call the API, and a custom `RBACMiddleware` that checks user roles on each request to protected endpoints. This centralizes cross-cutting concerns so every request passes through these layers. In our project, the RBAC middleware (or dependency) will inspect the request’s auth token and verify the user’s role is sufficient for the resource being accessed, returning a 403 Forbidden if not authorized.

* **Include Routers:** Finally, we include all API routers. For example:

  ```python
  from app.api.routers import auth, users, ml_model, dashboard, internal
  app.include_router(auth.router, prefix="/auth")
  app.include_router(users.router, prefix="/users", dependencies=[Depends(get_current_user)])
  app.include_router(ml_model.router, prefix="/model", dependencies=[Depends(get_current_user)])
  app.include_router(dashboard.router, prefix="/dashboard", dependencies=[Depends(get_current_user)])
  app.include_router(internal.router, prefix="/internal", dependencies=[Depends(get_current_admin_user)])
  ```

  This setup attaches each router with a URL prefix and any route-specific dependencies (e.g., requiring authentication). By grouping endpoints logically and using **FastAPI’s dependency injection**, we avoid repeating security checks in each route definition. For instance, attaching `Depends(get_current_user)` at the router level ensures all routes under `/users`, `/model`, etc., require a valid JWT and user authentication.

**FastAPI Events & Lifecycle:** The use of startup events in `main.py` also ties into the **request/response lifecycle**. By preloading resources (like ML models or an HTTPX client) at startup and cleaning them up at shutdown, we manage resource lifetimes at the app level. FastAPI’s event system or lifespan context manager covers the entire application lifespan, running code **before the app starts accepting requests and after it stops**, which is ideal for expensive setup/teardown. For example, we load our ML model in a startup hook so that subsequent prediction requests can use it directly from memory, and we close database sessions or threadpools in shutdown to free resources.

In summary, `app/main.py` orchestrates the application configuration – it initializes the FastAPI app, registers all routes, sets up global middleware, and uses **startup events** to prepare the environment. This is the heart of the project where everything comes together.

## API Routers and Endpoints (`app/api`)

The **API layer** is organized into multiple routers under `app/api/routers`. Each router module focuses on a specific domain or feature set, making the API modular and easier to navigate. This segmentation also aligns with potential team responsibilities (e.g. different team members can own the auth vs. model serving logic). Key routers include:

* **Authentication (`auth.py`):** Contains endpoints for user signup, login, token refresh, etc. For example, a POST `/auth/login` endpoint verifies user credentials and returns a JWT. This router likely uses FastAPI’s OAuth2 password flow and depends on utility functions in `core/security.py` to hash/verify passwords and generate JWT tokens. If using the third-party library **fastapi-users**, we would integrate its router or manager here to handle the heavy lifting of auth (fastapi-users can auto-generate routes for registration, login, forgot password, etc.). This module ensures the rest of the API can rely on a stable authentication mechanism. (We’ll discuss JWT and security details in a later section.)

* **Users (`users.py`):** Provides user management endpoints (e.g., GET `/users/me` to fetch profile, PATCH `/users/{id}` to update profile, etc.). These require a valid JWT auth via dependency `get_current_user`. They might also demonstrate **role-based access** by, for example, only allowing admins to list all users. This router interacts with the database (via `models/user.py` and possibly `services/auth_service.py`) to perform CRUD on user accounts. Pydantic **schemas** in `schemas/user.py` define the shape of user data for requests and responses (for instance, `UserCreate`, `UserRead`, `UserUpdate` models), decoupling the API from internal DB models.

* **ML Model Serving (`ml_model.py`):** Hosts endpoints related to the machine learning model. A typical endpoint might be `POST /model/predict` which accepts an input payload (e.g. features for prediction) defined by a Pydantic schema in `schemas/ml.py`. This endpoint will **asynchronously** call the ML inference logic from `services/ml_service.py` and return the prediction result. Because model inference can be CPU or GPU intensive, this router also handles job submission and status. For example, a `POST /model/predict_async` could enqueue a long-running inference task (using a BackgroundTasks or Celery) and immediately return a task ID, and a complementary `GET /model/status/{task_id}` endpoint can return the task’s status or result. All endpoints here are defined with `async def` to leverage FastAPI’s asynchronous performance for I/O-bound operations (like reading input, waiting on model I/O, or database calls). **Dependency injection** is used to provide any needed resources to these endpoints – e.g., injecting a database session if predictions are logged, or injecting the ML model object if loaded at startup. By structuring model-serving logic in its own router, we encapsulate ML-specific code and clearly demarcate it from other API concerns.

* **Dashboard (`dashboard.py`):** Provides data for the real-time dashboard. This may include standard GET endpoints (e.g., `GET /dashboard/metrics` to retrieve aggregated stats or recent predictions for display), as well as **Server-Sent Events (SSE) or WebSocket endpoints** for pushing real-time updates. For instance, a `GET /dashboard/stream` could use **SSE** to continuously send metrics updates to the frontend, or a `@app.websocket("/dashboard/ws")` endpoint could push notifications when a long-running ML job status changes. FastAPI’s WebSocket support allows defining an async function with `WebSocket` parameter and using `await websocket.send_text(...)` or `send_json(...)` to push data. In this project, we might use a **WebSocket connection to stream model inference progress or logs** to the dashboard UI. When a client connects, the endpoint will accept the connection and register it with a **connection manager** (an object that keeps track of active WebSocket clients). Then, as background tasks (or the ML service) produce new status messages or results, the manager broadcasts these to all subscribed dashboard clients. This enables truly real-time updates in the dashboard. (More on WebSocket implementation in the *Real-Time Updates* section below.)

* **Internal Tooling (`internal.py`):** Contains endpoints intended for internal or admin use only. These might include administrative tasks such as triggering a model re-training, clearing a cache, viewing system health stats, or managing user roles. Such endpoints are protected by stricter security (e.g., allowing only admin-role users via dependency `get_current_admin` which checks the JWT’s role claim). Organizing them in a separate router (with a prefix like `/internal` or `/admin`) makes it easy to apply common security requirements. For example, we can include this router with `dependencies=[Depends(get_current_admin_user)]` so that *every* internal route automatically enforces RBAC. Internal routes might also consume external APIs or services. For instance, an internal endpoint could fetch data from an external ML monitoring service or hit an external API to purge a CDN cache. In doing so, it would utilize the **external API client** from our `services/external_api.py` module.

Each router uses **FastAPI’s dependency injection** (`Depends`) to cleanly access shared components like database sessions, current user info, or external service clients. By placing common dependency functions in `api/dependencies.py`, we avoid repetition. For example, a `get_db()` dependency can be defined once and reused in any endpoint that needs a database session: FastAPI will automatically call it (getting a session from `db/session.py`) and inject the session into the path function. Likewise, `Depends(get_current_user)` can decode a JWT and provide the `User` object to endpoints without cluttering the endpoint logic. This showcases FastAPI’s powerful but intuitive DI system.

**Asynchronous and Non-blocking I/O:** All routers are implemented with async functions to support FastAPI’s non-blocking concurrency. This is especially important when our endpoints perform I/O such as database queries or calls to external APIs. For example, if the dashboard endpoints fetch data from PostgreSQL, we can use an async DB client (like `asyncpg` via SQLAlchemy 2.0 or Tortoise ORM) so the server can handle other requests while waiting for DB results. Similarly, if we call an external API (say to get additional data for a prediction), we use the async HTTP client **httpx** to avoid blocking. **HTTPX** is a preferred library for external calls in FastAPI apps because it supports async/await and is even installed by default with FastAPI. We configure an `httpx.AsyncClient` (possibly at startup, stored in `app.state` or passed via dependency) to handle external requests efficiently. By following this pattern, our API can scale to many concurrent connections, crucial for real-time dashboards and multiple simultaneous model inferences.

In summary, the `app/api` package defines a **clear separation of API concerns** through multiple routers. Each router file is focused (single responsibility), uses dependency injection for clean access to shared resources, and supports asynchronous operation. This modular design allows the ML engineer to navigate and extend the API easily – e.g., adding a new router for another model or feature – without monolithic code. It also aids testing, as each router’s functionality can be tested in isolation by including only that router in a test app.

## Machine Learning Model Serving and Background Tasks

Serving an ML model in production requires careful handling because inference might be resource-intensive or long-running. Our project demonstrates two patterns: **synchronous prediction for fast models** and **background processing for heavy jobs**.

**Model Loading:** The ML model (or models) is loaded once at application startup. In `services/ml_service.py`, we might define a function to load the model from disk (or from a registry) and perhaps wrap it in a class with a predict method. Using the FastAPI startup event, we ensure this load happens only one time. As the FastAPI docs illustrate, *“loading the model can take time, so you don’t want to do it for every request”*. Instead, we load it in the startup event and store it in memory (e.g., as a global `model` or in `app.state.model`). This way, when a request hits the prediction endpoint, the model is readily available in memory. This technique reduces per-request latency and avoids repeated expensive initialization.

**Prediction Endpoint (Sync vs Async):** For relatively quick predictions (tens of milliseconds to a few seconds), an **async endpoint** in `ml_model.py` can directly call the model and return the result. We use Pydantic `schemas.MLPredictionRequest` and `schemas.MLPredictionResponse` to validate input and format output. Because the model might be a CPU-bound operation, it’s important to consider that even with `async def`, CPU-bound work can block the event loop. For lightweight models or using small batches, this is usually fine. But for heavier computations or if using a GPU (which might block on CUDA calls), we might offload inference to a **threadpool** (using Starlette’s `run_in_threadpool`) or hand off the job to a background worker.

**Background Tasks:** FastAPI’s built-in `BackgroundTasks` utility allows us to execute code after the response is sent. In `ml_model.py`, we could accept a request and immediately return a task ID while a background task processes the input and stores the result (perhaps in Redis or the DB). However, FastAPI’s background tasks run in the same server process after sending the HTTP response. For truly long jobs or more robust handling, integrating a **task queue** like **Celery or RQ (Redis Queue)** is a best practice. In our structure, `services/tasks.py` could define Celery tasks (if using Celery with Redis broker) or RQ workers. The internal logic might enqueue a job to Redis and immediately return a job identifier to the client. A separate worker process (started via Celery/RQ) would then pick up the job, run the heavy model inference, and perhaps update a status in the database or notify via WebSocket when done.

By using asynchronous background jobs, we ensure that long-running tasks **don’t block the main server** and degrade API responsiveness. As one article notes, running tasks that take a long time (complex calculations, report generation, etc.) asynchronously **“can improve the overall performance and responsiveness of your application”**. Our project embraces this: for example, a **/model/train** endpoint (to retrain or fine-tune the model on new data) would likely enqueue a training task instead of trying to train within an HTTP request.

**Communicating Task Status:** To keep clients informed, we leverage **Redis** (via `db/redis.py`) as a fast in-memory store to track job states or partial results. When a background job starts, we could store an entry in Redis like `status:job123 = "running"`. The `/model/status/{id}` endpoint (in `ml_model.py`) can quickly fetch this from Redis to return the current status or result. Redis is well-suited for this ephemeral data and caching use case due to its speed. Additionally, if using WebSockets for real-time updates, the background job can push progress messages to connected clients (more on this below).

**Integration with Services:** The `services/ml_service.py` serves as the bridge between the API layer and the ML model logic. This might include functions like `predict(input_data) -> result` and possibly `train_model(async=True)` to dispatch background training. This separation means the API endpoint handler simply calls `result = ml_service.predict(data)` or triggers `ml_service.train_model()` without needing to know how the model is implemented. It makes testing easier (we can test `ml_service` functions independently of FastAPI) and allows reuse (e.g., internal tools could call the same service functions). If external APIs are needed as part of prediction (for example, maybe calling an external NLP API or a computer vision microservice), `ml_service.py` or an `external_api.py` would use **httpx** to fetch those results asynchronously and combine with the model’s output. Using httpx ensures such external calls don’t block our event loop.

In summary, the model serving component is designed to handle both **fast and slow ML tasks** gracefully. By using startup loading, async endpoints, background tasks, and possibly a task queue, we cater to the needs of real-world ML workloads. This architecture ensures that whether a prediction takes 50ms or 50 seconds, the system remains responsive. The heavy lifting is done in the background, and clients can either poll for results or receive push notifications via WebSockets, as we’ll discuss next.

## Real-Time Updates with WebSockets (Dashboard Notifications)

Real-time feedback is a key part of our project’s dashboard. FastAPI natively supports WebSockets, which enable **bidirectional communication** between the server and clients (e.g., a web frontend). We use this to push live updates such as **model inference status, logs, or new metrics** to connected dashboard users in real time.

**WebSocket Endpoint:** In the `dashboard.py` router (or a dedicated `websocket.py` module), we define a WebSocket route, for example:

```python
from fastapi import WebSocket, WebSocketDisconnect

@router.websocket("/ws")
async def websocket_endpoint(ws: WebSocket, token: str):
    # auth: token can be a query param used to authenticate the user
    await ws.accept()
    user = authenticate_token(token)  # custom function or Depends injection
    if not user:
        await ws.close(code=1008)  # policy violation/unauthorized
        return
    connection_manager.subscribe(user.id, ws)
    try:
        while True:
            _ = await ws.receive_text()  # (Optional) receive messages from client
            # We might not expect messages from client, as this is mainly for server push.
            # This loop keeps the connection open.
    except WebSocketDisconnect:
        connection_manager.unsubscribe(user.id, ws)
```

This pattern accepts the WebSocket handshake, possibly performs a lightweight auth (ensuring only logged-in users connect), then uses a `connection_manager` to keep track of the client. The `connection_manager` (which could be implemented in `services/dashboard_service.py`) maintains a mapping of active connections, potentially keyed by some topic or user. In a scenario with multiple types of updates, we might key by a job ID or user ID to target messages.

**Connection Manager:** The connection manager typically has methods like `subscribe(client)` and `broadcast(message, target=...)`. In our ML use case, when a background task updates the status of a job, it can call `connection_manager.broadcast(status_update, target=job_id)` to send a message to all WebSocket clients interested in that job (or to a specific user’s dashboard). Under the hood, this manager iterates over the set of `WebSocket` objects and calls `await ws.send_json(data)` for each. FastAPI’s WebSocket support handles each send in an awaitable fashion.

By structuring it this way, **all connected clients receive real-time updates** relevant to them, as soon as the server emits them. This is exemplified in other FastAPI real-time apps; for instance, a real-time polling app uses a WebSocket route to notify all clients when a poll gets a new vote. In our project, clients could be notified that “Job 123 is 50% complete” or “New data point X arrived for the dashboard chart” without needing to constantly poll the server.

**Server-Sent Events (Optional):** An alternative to WebSockets is using Server-Sent Events (SSE) for one-way streaming. The included example in this project (as seen in `dashboard.py` with a `@router.get("/stream")` endpoint) uses **sse-starlette** integration to stream updates. SSE is simpler (client just keeps an HTTP connection open), but WebSockets are more flexible (bi-directional and often with lower overhead for frequent messages). Our structure can accommodate both: an SSE endpoint for simpler clients and a WebSocket for interactive clients.

**Background Task Integration:** The background tasks producing updates (like a Celery worker performing training) need a way to send messages to the WebSocket connections. There are a few approaches:

* The background job can use Redis pub/sub or a message broker to communicate with the web process. We could run a small coroutine in FastAPI that subscribes to a Redis channel and, on receiving a message, forwards it via the WebSocket manager to clients. This decouples the worker and web app (common in distributed deployments).
* Simpler: if using FastAPI’s internal `BackgroundTasks`, since those run in the same process, they can directly call the connection manager (which is in memory) to send messages. We must be careful with thread-safety if doing this, but since FastAPI’s background task runs after the response, it can still utilize the asyncio loop to send on websockets.

In the project structure, we could place WebSocket-specific logic in `dashboard_service.py` or a new `services/notification_service.py`. This might include the connection manager class and helper functions to format and dispatch messages. We also configure our `app` with any necessary **CORS** or WebSocket specific middleware if the frontend is hosted on a different origin (FastAPI’s CORSMiddleware doesn’t affect WebSockets, but we need to ensure network and authentication considerations are handled).

**Front-End Consideration:** The real-time dashboard (though not part of backend code) would connect to the WebSocket like:

```js
const socket = new WebSocket("wss://api.example.com/dashboard/ws?token=<JWT>");
socket.onmessage = (event) => {
    const data = JSON.parse(event.data);
    // update UI with new data
};
```

By sending the JWT as a query param (or using a subprotocol for auth), the backend can authenticate the connection. All subsequent messages from server to client are handled in the `onmessage` callback, updating charts or logs instantly.

The result is a **responsive dashboard** experience. As soon as an ML job finishes or a new prediction comes in, the UI updates in real time. FastAPI’s efficient async capabilities and the design of the WebSocket endpoint ensure this can scale to multiple concurrent clients. In effect, we have a mini pub-sub system where the FastAPI app is the broker broadcasting updates.

## Database Integration (PostgreSQL and Redis)

This project uses both a **relational database (PostgreSQL)** and a **NoSQL key-value store (Redis)**, showcasing how to integrate multiple data sources:

**Relational DB with SQLModel/SQLAlchemy:** We choose SQLModel (built on SQLAlchemy) for defining our relational models and interacting with PostgreSQL. SQLModel seamlessly combines Pydantic models with SQLAlchemy models, making it a “perfect match” for FastAPI applications. In `app/models/`, we define classes like `User`, `Item`, etc., using SQLModel or SQLAlchemy’s declarative base. For example, `models/user.py` might have:

```python
from sqlmodel import SQLModel, Field, Relationship

class User(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    email: str = Field(index=True, unique=True)
    hashed_password: str
    role: str = Field(default="user")  # e.g., "admin" or "user"
    is_active: bool = Field(default=True)
```

This defines a table and also serves as a Pydantic model for data exchange (with some limitations). We often also create separate Pydantic models in `schemas/` for read/write (to avoid exposing `hashed_password`, for example).

Database connection and session management is handled in `app/db/session.py`. Here we configure SQLAlchemy with the PostgreSQL URL (read from `core/config.py`). If using SQLModel, we create an engine:

```python
engine = create_engine(settings.database_url, echo=False)
```

and a session local:

```python
SessionLocal = sessionmaker(bind=engine, class_=Session)  # for sync
```

For **async** support, we could use `create_async_engine` and `async_sessionmaker` with `asyncpg`. The project might opt for synchronous ORM sessions handled in threadpool (which FastAPI can do implicitly when calling sync functions) or fully async DB queries. Either way, the `get_db` dependency provides a session to endpoints:

```python
def get_db():
    with SessionLocal() as session:
        yield session
```

FastAPI will call this once per request and make sure to close the session after (especially if using `yield` syntax for dependencies). This pattern is recommended in FastAPI docs and ensures each request gets an isolated session.

We also leverage **SQLModel’s integration** to create tables on startup if needed. For example, in `session.py` we might define:

```python
def create_db_and_tables():
    SQLModel.metadata.create_all(engine)
```

and call this in a startup event so that the database schema is ensured (useful in dev/test environments). In production, one might run migrations instead (the `db/migrations/` folder can contain Alembic migration scripts for schema changes).

**Using the Database:** The **CRUD operations** on the database can be abstracted into separate modules or done inline in service functions. For clarity, you might create a `app/services/user_service.py` that contains functions like `create_user(session, user_data)` or `get_user_by_email(session, email)`. These encapsulate DB logic (queries using SQLAlchemy ORM or SQLModel select). This aligns with the idea of a *repository* or *CRUD layer* often seen in FastAPI projects. Whether separated or not, all DB interactions occur via the session provided by dependencies, keeping them consistent and easy to test (we can override the `get_db` dependency in tests to use a SQLite memory DB, for example).

FastAPI doesn’t force any specific database library – it’s compatible with any, as noted in the docs (you can use any SQL or NoSQL database with FastAPI). Here we choose PostgreSQL for its reliability and use SQLModel for convenience, but one could swap in another ORM or even raw queries if needed.

**Redis Integration:** Redis serves two purposes in our project:

1. **Caching:** We can cache frequent read results (like expensive queries or model predictions) to speed up the API. For example, when a client requests a prediction for input X, we might first check Redis cache if that result was recently computed, to avoid recomputation. This is a typical performance strategy – caching improves response times and reduces load.
2. **Message Broker / Shared State:** As mentioned, we utilize Redis as a lightweight message broker or state store for background tasks. When a background job updates progress, it could publish messages on a Redis channel or set keys that the FastAPI app reads. If using Celery, Redis (or RabbitMQ) is required as the broker to queue tasks.

In `app/db/redis.py`, we configure a Redis client, likely using `redis-py` (`pip install redis`) or `aioredis` for async usage. For example:

```python
import redis
redis_client = redis.Redis(host=settings.redis_host, port=settings.redis_port, db=0)
```

For async usage, `import aioredis` and do `redis = await aioredis.from_url("redis://localhost:6379")` inside a startup event (and store in app state or a global). The decision of sync vs async Redis client depends on whether we perform Redis operations in the request thread (synchronously) or want to await them. Since Redis operations are usually fast, synchronous usage in a threadpool is fine, but pure async can be a bit more efficient if available.

**Dependency Injection for Redis:** Similar to DB, we can provide Redis via dependency. For instance, define `get_redis()` that yields the `redis_client`. Then an endpoint can declare `redis=Depends(get_redis)` to use it. This approach abstracts the data layer nicely – our endpoint code doesn’t need to know details of connecting to Redis.

**Use Cases in Code:** To illustrate, suppose our `/model/predict` endpoint wants to cache results. The flow might be:

* Check `redis_client.get(f"prediction:{input_hash}")`. If exists, return the cached result.
* If not, compute the prediction via model, store result in cache with an expiration: `redis_client.set(f"prediction:{input_hash}", result_json, ex=3600)`.
* Return result to client.

Another use: the `/dashboard/metrics` endpoint might aggregate data from SQL (like number of predictions today) and cache it in Redis for a short time to reduce DB load if the dashboard refreshes often.

By integrating PostgreSQL and Redis side by side, our project demonstrates a **polyglot persistence** approach: using the right tool for each job (SQL for structured data and transactions, Redis for fast ephemeral operations). FastAPI’s design makes it straightforward to incorporate both. It doesn’t impose an ORM; it even notes you can use any database or none. The key is to keep the integration **modular**:

* The `app/core/config.py` holds the configuration for both databases (e.g., `database_url`, `redis_url`).
* `app/db/` holds the initialization for each.
* Dependencies provide sessions/clients to whichever part of the app needs them.

This way, our endpoints and services remain agnostic about how data is stored, and switching out or scaling the databases can be done with minimal changes localized to these modules.

## Security: Authentication with JWT and Role-Based Access Control (RBAC)

Security is critical, especially since we have internal admin tools and potentially sensitive model endpoints. We implement a **JWT-based authentication** scheme and enforce **role-based access control** to restrict actions.

**JWT Authentication:** We opt for a stateless auth model using JSON Web Tokens (JWT) as it’s a common, scalable approach for APIs. In `app/core/security.py`, we configure the JWT settings:

* Generate a secure secret key (from config or environment).
* Choose signing algorithm (HS256 for symmetric or RS256 for asymmetric).
* Set token expiration (e.g., access tokens valid for 15 minutes, refresh tokens for a day).

FastAPI’s docs provide a blueprint for implementing OAuth2 password flow with JWTs. We’ll follow this pattern:

* Use `OAuth2PasswordBearer` as a dependency to retrieve the `Authorization: Bearer <token>` header.
* Write a utility `decode_jwt(token)` to validate and decode the JWT (using PyJWT or python-jose).
* Provide a dependency `get_current_user` that:

  1. Depends on OAuth2PasswordBearer (to get the token string).
  2. Decodes the JWT, verifies signature and expiry.
  3. If valid, retrieves the user from the database (we might store user ID or email in the token’s subject claim).
  4. If user exists and is active, return the user object; else raise `HTTPException(401)`.

The `auth.py` router will have a login endpoint that uses `authenticate_user(email, password)` (checks DB for user and verifies password hash using PassLib bcrypt) and then creates a JWT:

```python
access_token = create_jwt({"sub": user.id, "role": user.role})
return {"access_token": access_token, "token_type": "bearer"}
```

This token is then used by clients for subsequent requests. We store minimal info in JWT (user ID and role), relying on DB lookups for details if needed. The stateless token means we don’t need a shared session store and can horizontally scale the API easily.

We also consider using **fastapi-users** library for a quicker solution. FastAPI-Users can generate all these routes and even handle refresh tokens, email verification, etc. If integrated, our `auth.py` could simply include the routes provided by fastapi-users. This library works with SQLModel and Pydantic, and it manages JWT auth backends internally. As one Medium article noted, *fastapi-users is a library for user management in FastAPI* that can save us from reimplementing standard auth. We might use it to avoid boilerplate, but we remain aware of how things work under the hood for learning purposes.

**Password Storage:** We never store plain passwords. `core/security.py` will use PassLib to hash passwords (e.g., Bcrypt). The `User` model has a `hashed_password` field. During registration, we hash the provided password; during login, we hash the input and compare with stored hash. This is standard secure practice.

**RBAC (Role-Based Access Control):** With JWT containing the user’s role, implementing RBAC becomes straightforward. We define roles (e.g., “user”, “admin”, maybe “manager” if needed). There are two main ways to enforce RBAC:

1. **Per-route dependency:** For admin-only endpoints, we can have a dependency like:

   ```python
   def get_current_admin_user(current_user: User = Depends(get_current_user)):
       if current_user.role != "admin":
           raise HTTPException(status_code=403, detail="Not enough privileges")
       return current_user
   ```

   Then apply `Depends(get_current_admin_user)` on any router or endpoint that requires admin rights. This way, if a non-admin calls it, they get a 403. This fine-grained control can be customized per route (e.g., other roles for other endpoints).
2. **Global middleware:** As mentioned earlier, we can implement RBAC in a middleware that runs before request processing to check permissions. For example, `core/rbac.py` could define a `BaseHTTPMiddleware` that inspects the path and method, and the user’s role, to decide if it should proceed. The Medium article demonstrated a simple mapping of roles to allowed actions on resources, and a middleware that checks this map for each request. The middleware approach centralizes authorization logic – ensuring each request is vetted – but it requires a well-defined mapping of URLs or resource names to roles/permissions. It may also be overkill if most of the app isn’t role-differentiated. In our case, we might use a **hybrid approach**: simple dependency checks for obvious role gates (like admin endpoints), and possibly a middleware to log any forbidden access attempts or ensure no unauthenticated request goes through to data-modifying endpoints.

Given our project’s scope:

* The **internal router** is restricted to admins (we use dependency injection to enforce that).
* The **dashboard and model endpoints** require at least a normal authenticated user (enforced by `get_current_user` dependency).
* Specific actions (if we had multiple roles beyond admin/user) could be decorated with custom dependencies (e.g., `RequiresRole("data_scientist")`). We can build such a dependency easily using the pattern above.

For a more advanced setup or large organization, one could integrate a library like **Casbin** for RBAC or an external service (Auth0, Permit.io, etc.), but that’s optional. The question specifically asks for JWT and RBAC within FastAPI, so we stick to that.

**Example of RBAC in action:** Suppose we have an endpoint to **retrain the model**: `POST /internal/retrain`. We only want users with role “admin” or “ml-engineer” to trigger this. In `internal.py`:

```python
@router.post("/retrain")
async def retrain_model(background_tasks: BackgroundTasks, current_user=Depends(get_current_admin_user)):
    # current_user will only be set if an admin; else 403 is raised before this line
    background_tasks.add_task(ml_service.start_retraining)
    return {"detail": "Model retraining started"}
```

This demonstrates stacking dependencies: `get_current_admin_user` itself calls `get_current_user` then does a role check. The result is that unauthorized callers never execute the function body. This is clean and expressive – the function signature declares its security requirements.

**Protecting WebSockets:** We also protect our WebSocket connections. While the WebSocket handshake doesn’t use the normal dependency system, we can manually authenticate by requiring a token parameter or cookie. In our `websocket_endpoint`, as shown, we pass a `token` query param and then authenticate it. If invalid, we refuse the connection. This ensures that only authorized users receive sensitive real-time updates.

**Summary of Security Best Practices:**

* Use **environment variables** (via `config.py`) to store secrets like JWT secret keys. Do not hard-code these.
* Use strong password hashing (e.g., Bcrypt with salt from PassLib).
* Use JWT access tokens for stateless auth. Optionally, implement refresh tokens if you want long sessions without re-login.
* Utilize FastAPI dependencies to enforce security at the highest level possible (router or app level) so that it’s easy to reason about which parts of the API are open vs. protected.
* Log security events (login failures, access denials) for audit – possibly via the middleware logging.

By following these practices, our FastAPI application ensures that only authenticated and authorized users can access the various features. The combination of **JWT for authentication** and **role checks for authorization** is a robust solution common in production APIs, and FastAPI’s tools make it relatively straightforward to implement correctly. The end result is an API that is secure by design, while still convenient for legitimate users (e.g., the dashboard can use the JWT it got at login to open a WebSocket and receive updates).

## Configuration Management (Pydantic Settings)

Managing configuration (database URLs, API keys, debug flags, etc.) in a scalable way is addressed in `app/core/config.py`. We use **Pydantic Settings** (based on Pydantic’s BaseSettings) to define a strongly-typed config class that reads from environment variables or a `.env` file. This provides **type safety** and centralization of config.

For example, `config.py` might have:

```python
from pydantic import BaseSettings

class Settings(BaseSettings):
    app_name: str = "FastAPI ML Service"
    environment: str = "dev"
    database_url: str
    redis_url: str
    jwt_secret: str
    model_path: str = "./models/model.bin"
    # ... other config values

    class Config:
        env_file = ".env"
```

This class automatically reads corresponding environment variables (e.g., `DATABASE_URL`, `REDIS_URL`, etc., case-insensitive by default) or loads from a `.env` file in development. We create a single instance `settings = Settings()` when the app starts (often at module import time in `core/config.py`). Because Pydantic will load and validate the values, we catch config issues early (like missing env vars or wrong types).

In `main.py`, we import this `settings` and use it:

```python
app = FastAPI(title=settings.app_name)
```

We also pass `settings` into other parts of the app as needed (though often, importing the already-initialized `settings` is fine since BaseSettings caches values).

Benefits of this approach:

* **Separation of secrets from code:** We can keep actual secrets (DB passwords, JWT secrets) out of the repository by using env files or actual environment variables in production. Our code just references `settings.jwt_secret`, for instance.
* **Multiple environments:** We could extend the Settings class to load different `.env` files or use environment variable prefixes for dev/staging/prod. Pydantic BaseSettings has features to support that. The class approach means we could have e.g. `SettingsDev`, `SettingsProd` subclasses or simply use environment variables to override values in each environment (12-factor app style).
* **Integration with Dependency Injection:** We might define a dependency to retrieve settings with caching. The FastAPI docs suggest using `@lru_cache` on a function that returns Settings. This way, if we inject `settings: Settings = Depends(get_settings)`, it will only initialize once and then reuse. In our case, since we typically import and use a single `settings` instance, we may not need to inject it everywhere – but for testability, having a dependency can be useful (we can override it in tests to simulate different config).

Using Pydantic in this manner **harnesses all its power** for validation (e.g., if a config value should be an int, it will coerce or error if not). It’s also convenient for complex config like nested settings (for example, if we had a section for third-party API credentials, Pydantic can model those as sub-classes).

**Best Practices:**

* **Do not commit actual secrets** – Instead commit a `.env.example` with placeholders. Developers can create their own `.env` for local runs, and in production, real secrets are injected via environment (Kubernetes secrets, etc.).
* **Use default values where appropriate** – Like reasonable defaults for timeouts or optional features, so the app can start with minimal env vars set.
* **Leverage Pydantic Validators** if needed – e.g., to ensure an `API_KEY` has a certain format or to derive one setting from another.

Overall, `core/config.py` gives us a single source of truth for configuration. The rest of the code imports from here rather than scattering constants around. This improves maintainability – if a configuration key changes, we update it in one place. It also aids testing: we can override settings (via monkeypatch or dependency override) to test behaviors under different configurations (like setting `settings.environment = "test"` might toggle certain features off).

## Testing Strategy and Practices

A maintainable project must include tests. We create a `tests/` directory (at the project root) with test modules mirroring the functionality of our app modules. For example:

* `test_auth.py` will test authentication flows (valid login, invalid login, token protection on various endpoints).
* `test_ml_model.py` will test the prediction endpoint (both success and failure cases, perhaps using a lightweight or dummy model).
* `test_dashboard.py` will test that the dashboard endpoints (like metrics) behave as expected, and possibly that WebSocket communications are working (Starlette’s `TestClient` can connect to WebSockets for testing).

**Testing Tools:** We use **Pytest** as the test framework. FastAPI provides a `TestClient` (based on httpx) to simulate HTTP calls to our app. For example:

```python
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

def test_login_success():
    resp = client.post("/auth/login", data={"username": "user@example.com", "password": "secret"})
    assert resp.status_code == 200
    data = resp.json()
    assert "access_token" in data
```

This spins up the app in testing mode and allows us to call endpoints as if a real client. We can test both typical cases and edge cases (e.g., 401 on wrong password).

**Dependency Overrides:** FastAPI allows overriding dependencies in tests. We can leverage this to, for instance, use an in-memory SQLite database instead of the real Postgres during tests. In a test fixture, we might do:

```python
from app.db.session import SessionLocal, engine, Base
# Create tables in a clean SQLite for testing
Base.metadata.create_all(bind=engine)

# Override get_db to use this test session
def override_get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

app.dependency_overrides[get_db] = override_get_db
```

This way, when tests run, any endpoint that calls `get_db` will get our test DB session (pointing to SQLite). Similarly, we could override `get_current_user` to easily simulate an authenticated user without having to obtain a real JWT for every test – though for end-to-end correctness, we might do a couple tests with real JWT generation.

**Isolated Unit Tests:** Not all tests need to be full API calls. We can and should directly test pure functions or services. For example, `services/ml_service.py` functions can be tested by injecting a fake model or known inputs. If `ml_service.predict` uses a global model, we might monkeypatch it in tests to use a dummy function that returns a constant. The services layer design makes such testing easier, because they are just Python functions/methods without inherent FastAPI context.

**Testing WebSockets:** We can test WebSocket logic using Starlette’s test client as well:

```python
with TestClient(app) as client:
    with client.websocket_connect("/dashboard/ws?token=validtoken") as ws:
        # Simulate receiving a message from server side
        # (This might require spawning a background task or trigger)
        result = ws.receive_json()
        assert result["event"] == "job_update"
```

This is more complex and might be considered an integration test. We might decide to test the connection manager logic separately as a unit (e.g., add and remove connections, broadcast to correct recipients) and assume Starlette’s WebSocket handling works.

**Continuous Integration (CI):** We recommend setting up a CI pipeline (GitHub Actions, GitLab CI, etc.) to run tests on each commit. This ensures that as we refactor or add features, nothing breaks unexpectedly.

**Linting and Code Style:** Alongside testing, maintaining code quality is important. Use linters/formatters (flake8, black, isort) to keep the code consistent. Possibly integrate these into pre-commit hooks or CI as well.

By investing in testing, we get confidence in our project. FastAPI’s design makes it easy to test in layers – from the bottom (unit tests for pure logic) to the top (API endpoint tests that ensure the whole stack works together). This project’s structure (with clear separation of concerns) directly benefits testing, because each piece can be mocked or tested in isolation. For instance, we could swap the real model with a fake in tests by overriding a dependency or monkeypatching `ml_service.model`. This way, tests remain fast and deterministic.

## Logging and Monitoring

Logging is configured in `app/core/logging.py` or as part of `core/config.py`. A robust logging setup helps in debugging and monitoring the application in production. We follow these practices:

* **Standard Python Logging:** We configure loggers for our app, possibly using `logging.config.dictConfig` if we have a complex setup. At minimum, we set the log level from config (debug in dev, info/warning in prod) and log format to include timestamps and request IDs if available.

* **Uvicorn/Access Logs:** Uvicorn, the ASGI server for FastAPI, provides access logs for requests. We can either leave these on or integrate them with our logging. Often we might disable default access logs and use our own middleware to log requests with more context (like user ID or execution time).

* **Request Logging Middleware:** A simple example in `core/middleware.py` could use `time.time()` at request start and log after response is sent:

  ```python
  import time
  from starlette.middleware.base import BaseHTTPMiddleware

  class LoggingMiddleware(BaseHTTPMiddleware):
      async def dispatch(self, request, call_next):
          start = time.time()
          response = await call_next(request)
          duration = time.time() - start
          logger.info(f"{request.method} {request.url.path} - {response.status_code} ({duration:.3f}s)")
          return response
  ```

  We add this via `app.add_middleware(LoggingMiddleware)` in `main.py`. This gives us an audit of all requests and their latency, which is vital for performance monitoring and debugging issues.

* **Model Inference Logging:** In `ml_service.py`, we might log important events like "Model loaded", "Received prediction request with parameters X", "Returned result Y in Z seconds". These logs (at debug/info level) help trace the behavior especially when something goes wrong with model outputs.

* **Security Logging:** We log authentication successes/failures and authorization denials. For instance, in `get_current_user`, if a token is invalid, we log a warning with the token identifier or user info (if any). If `get_current_admin_user` blocks a request due to insufficient role, we log that incident (possibly with user ID and attempted action). This provides an audit trail for security-relevant events.

* **External API Calls:** If our service calls out to external APIs via `httpx`, we log those calls and outcomes too. Often including the external endpoint and response status is helpful for diagnosing integration issues.

* **Monitoring and Metrics:** Although not explicitly asked, it’s worth noting that for a production ML service, we’d integrate with monitoring solutions. We could use Prometheus to track metrics like request rate, latency, etc., or use an APM tool. FastAPI can integrate with Starlette’s middleware for metrics or third-party libs (e.g., `prometheus-fastapi-instrumentator`). Our structure could include a `metrics.py` in core or middleware to expose a `/metrics` endpoint if needed.

The configuration for logging (log level, log destination) is driven by config (environment variables). For example, in `Settings`, we might have `log_level` and `log_to_file` options. In development, we log to console with debug level; in production, maybe to both console and a file (or JSON logs to stdout for centralized logging systems). Python’s logging is flexible enough for all that, and structuring it in `core/logging.py` centralizes the setup.

**Example:** We set `logging.basicConfig(format="%(asctime)s %(levelname)s [%(name)s] %(message)s")` and then throughout the app, use `logger = logging.getLogger(__name__)` at the top of modules to log messages. This way, each module’s logs are namespaced by module name. In production, one could adjust logger levels (e.g., set `uvicorn.error` to WARNING to reduce noise, etc.).

By implementing thoughtful logging, we ensure that when the ML model behaves unexpectedly or performance slows down, we have the information needed to investigate. Coupling this with the testing and structured design, the project is not only well-architected but also maintainable in the long run, when real users and data are involved.

## Utilizing Third-Party Packages and Extensions

Throughout the design, we’ve hinted at various libraries that complement FastAPI:

* **SQLModel** (or SQLAlchemy) for database ORM – chosen for its synergy with FastAPI/Pydantic.
* **Pydantic Settings** for config – leveraging Pydantic’s power for environment management.
* **httpx** for external HTTP calls – an async HTTP client that is recommended for FastAPI apps. Our `services/external_api.py` uses httpx AsyncClient to call external services without blocking.
* **fastapi-users** for authentication – an optional utility to add on, which can save time by providing a pre-built user model, registration, login, and JWT handling. In a project to “explore and master FastAPI”, one might first implement auth manually (to learn the ropes) and then consider how fastapi-users could replace that code for efficiency.
* **uvicorn\[standard]** – for running the app. We’d likely include uvicorn as a dependency and in development use `uvicorn app.main:app --reload`.
* **Celery + Redis** – if we go the route of a robust background worker, Celery is a de-facto choice. Setting up Celery would involve creating a `celery_app = Celery(broker=settings.redis_url)` and tasks in `services/tasks.py`. While integration adds complexity (you need a separate worker process), it’s worth it for truly long jobs and is common in production systems. We ensure our design accommodates this by decoupling immediate API response from the actual job processing.
* **Test and Dev tools**: We’d list `pytest` for testing, and maybe tools like `httpx` (for TestClient which is httpx-based), and formatting/linting libraries in the dev dependencies.

Our `pyproject.toml` or `requirements.txt` will reflect all these. It’s useful to separate core dependencies vs. dev (e.g., pytest and black in a dev section).

Finally, documentation is key for a project meant for learning and mastering. We’d maintain a good `README.md` and maybe use automated docs:
FastAPI automatically generates **OpenAPI docs** and a Swagger UI at `/docs`. With our structured routers (and using Pydantic models for request/response), those docs will be rich and accurate. For internal tooling, we might keep those endpoints out of public docs (FastAPI allows router include with `include_in_schema=False` for internal ones if desired).

By following these practices and utilizing these utilities, the project is not only an advanced playground for FastAPI features but also a solid template for real-world applications. It demonstrates how to integrate a wide range of FastAPI capabilities – **from dependency injection to WebSockets** – in a cohesive, scalable architecture.

## Conclusion

The structure and approaches described above provide a **comprehensive FastAPI project setup** tailored to a machine learning scenario. We combined:

* **Robust project organization** (clear modules for API, core config, models, schemas, services, etc.).
* **Advanced FastAPI features** like dependency injection for clean code, event handlers for startup (loading ML models and establishing connections), custom middleware for logging and RBAC, and WebSockets for real-time communication.
* **Asynchronous design** ensuring high performance, using async endpoints and tasks so the app can handle many concurrent requests and push updates without waiting on slow operations.
* **Multi-database integration** to use the best storage technologies for each purpose (PostgreSQL for persistent relational data, Redis for caching and background job coordination).
* **Security** via JWT auth and role-based permissions, to protect both public and internal endpoints effectively.
* **Best practices** in configuration management, testing, and logging, which are crucial for maintainability and observability of the system.

By adhering to this structure, the project remains **clean, scalable, and maintainable** as it grows. New features can be added as new routers or services without tangling the existing code. For a Machine Learning Engineer, this setup offers a playground to implement model serving and monitoring features while following software engineering best practices. It provides a blueprint not just for a one-off project, but for a sustainable application that can evolve from dev environment to production deployment confidently.

In essence, this project template helps bridge the gap between building a quick ML prototype and engineering a production-ready ML service using FastAPI – enabling exploration of FastAPI’s capabilities and mastery of its advanced features in a realistic, hands-on manner.

**Sources:**

* Abdul Rahman, A. (2024). *Structuring FastAPI Projects: Best Practices for Clean and Scalable Code* – Directory structure example.
* Parasuraman, M. (2024). *FastAPI Project Structure Best Practices* – Importance of organized, scalable code structure.
* FastAPI Documentation – **Events, Dependencies, and Security** – Guidelines on startup events, DI usage, and middleware for auth.
* Medium Articles – *FastAPI Users, FastAPI Websockets, HTTPX Usage* – Demonstrations of integrating JWT auth libraries, implementing WebSocket updates, and calling external APIs asynchronously.
* Akintola, O. (2024). *Asynchronous Job Processing using Redis Queue and FastAPI* – Benefits of background task queues for long-running jobs.
* FastAPI Official Docs – **SQLModel and Pydantic Settings** – Using SQLModel for DB and Pydantic for configuration management.
